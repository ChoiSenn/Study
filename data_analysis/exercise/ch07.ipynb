{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVwcNG9qoaLlTwMIiDqUlM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChoiSenn/Study/blob/main/data_analysis/exercise/ch07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 분석과 머신러닝**\n",
        "## 2022-04-27  머신러닝 기초 : 사이킷런과 선형 회귀\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "fTcnKNgdvD-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 경험을 통해 학습하는 인간을 통해 지능을 정의\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**지능**이란, 인간이 태어나서 성장하는 과정에서 겪은 다양한 경험과 시행착오를 통해 얻은 학습 능력, 추론 능력, 지각 능력, 언어이해 능력을 통칭한 것이다. 이러한 지능은 유전적 요인과 함께 **사회화와 학습의 과정에서 형성되는 것**으로 알려져 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "문제를 해결할 때 따라야 하는 일들을 지시하는 것을 **프로그램**이라고 한다.\n",
        "\n",
        "그러나 **풀이법을 설명하기 힘들거나 불가능한 경우, 이러한 프로그램을 만드는 일이 쉽지 않을 수도 있다**.<br>\n",
        "문제의 풀이법을 컴퓨터에 알려주지 않아도 컴퓨터가 스스로 잘하는 방법을 찾아내게 할 수 있을까? 컴퓨터가 **데이터를 기반으로 스스로 학습**할 수 있다면 더욱 복잡한 일을 맡길 수 있다. (ex. \"알파고\"에게 기보를 알려주어 바둑의 윈리를 학습하게 한 것.)\n",
        "\n",
        "<br>\n",
        "\n",
        "머신러닝에서는 동작 방식을 일일이 지시하는 프로그램을 설계하지 않는다. 대신 변경 가능한 **파라미터에 의해 동작이 결정되는 융통성있는 프로그램**을 만든다. 이것이 **모델**이다.<br>\n",
        "더 좋은 동작이 나오도록 파라미터를 변경하는데, 이 과정을 **학습**이라고 한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**인공지능이란 인간의 학습 능력과 추론, 지각 능력을 인공적으로 구현한 것**이다.<br>\n",
        "\n",
        "그 중, **기계학습 혹은 머신러닝은 반복적인 학습을 통해 분류 성능을 점점 향상시킬 수 있는 효율적인 알고리즘**이다. (기계학습 수행)<br>\n",
        "그리고 인간의 지능과 유사하게 다양한 상황에 따라 능동적으로 판단하고 자율적으로 주변환경과 상호작용하는 지능을 **범용 인공지능**이라고 한다."
      ],
      "metadata": {
        "id": "bQXS57G0vPs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 2. 머신러닝의 정의와 종류\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**머신러닝은 인공지능의 한 분야**이다.<br>\n",
        "톰 미첼은 그의 저서 \"머신러닝\"에서 머신러닝을 다음과 같이 정의했다.<br>\n",
        "> ### \"컴퓨터 프로그램이 어떤 작업 종류 T에 속한 작업을 수행하면서 경험 E에 따라서 P로 측정하는 성능이 개선된다면, 이 프로그램은 T와 성능 척도 P에 대해 경험 E로부터 학습을 한다고 말할 수 있다.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "우선 **해결해야 할 문제(작업)가 T**이다.<br>\n",
        "그리고 이 **일을 수행하는 동작을 P라는 성능 척도를 통해 평가**할 수 있어야 한다.<br>\n",
        "그리고 **지속적인 훈련 경험 E**를 통해 이러한 **평가의 점수를 더 나은 상태로 바꿀 수 있어야 한다.**\n",
        "\n",
        "<br>\n",
        "\n",
        "이러한 **머신러닝은 인공지능이라는 분야의 매우 중요한 영역으로 간주**된다. 그리고 요즘 각광받고있는 **인공신경망을 이용한 딥러닝 분야 역시 이 머신러닝의 한 분야**이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "머신러닝은 일반적으로 **기계에게 답을 알려주는 \"교사\"의 존재 여부에 따라 크게 지도 학습과 비지도 학습**으로 나누어진다. 그리고 **에이전트의 액션에 대한 보상을 학습하는 강화 학습**을 별도의 영역으로 다룬다.<br>\n",
        "\n",
        "- **지도 학습** : 지도 학습에서 컴퓨터는 \"교사\"에 의해 데이터와 정답의 역할을 하는 레이블을 제공받는다. 지도학습의 목표는 입력을 출력에 매핑하는 일반적인 규칙을 학습하는 것이다. 즉, A와 B를 구분할 때, 교사가 A인지 B인지 레이블링 된 데이터를 충분히 제공한 뒤에 학습을 하도록 하는 과정이 필요하다. 학습 단계에서 만들어진 예측 모델은 새로운 데이터에 대하여 이전의 학습을 바탕으로 A인지 B인지 맞히게 된다.<br>\n",
        "\n",
        "- **비지도 학습** : 지도 학습과는 달리 외부에서 정답(레이블)을 주지 않고 학습 알고리즘이 스스로 입력으로부터 어떤 구조를 발견하는 학습이다. 비지도학습을 사용하면 데이터에서 숨겨진 패턴을 발견할 수 있다. 비지도 학습의 대표적인 예가 군집화이다. 이는 주어진 데이터를 특성에 따라 둘 이상의 그룹으로 나누는 것인데, 이 특성을 구분하는 방법을 컴퓨터가 스스로 학습하는 것이다.<br>\n",
        "\n",
        "- **강화 학습** : 강화 학습은 에이전트, 환경, 액션, 보상과 상태라는 학습 데이터를 준다. 예를 들어 게임 캐릭터(에이전트)가 게임 환경에서 특정한 액션을 수행하고 이에 대한 보상을 통해 행동을 결정하는 정책을 바꾸어 나가는 방식이다. 게임과 같은 분야에서 높은 수준을 보이는 프로그램들이 이러한 방식으로 만들어진다."
      ],
      "metadata": {
        "id": "3PwpTh31xz7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 3. 회귀분석과 독립변수, 종속변수\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**회귀**란, 어딘가로 돌아간다는 것이다. <br>\n",
        "\n",
        "통계 분야에서 회귀라는 용어로 처음 사용한 사람은 **프랜시스 골터**인데, 그는 유전의 근거로 키가 큰 아버지의 자식은 키가 큰 경향을 가질 것으로 가정했다. 그러나 데이터를 통해 그렇지는 않다는 사실을 발견하였다. 그리고 아버지의 키가 작은 경우에도 그 자식들은 아버지와는 달리 평균 키에 가까운 경향을 보인다는 사실도 확인하였다. 그는 이러한 현상을 **평범성으로의 회귀**라고 하였다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**회귀분석은 대표적인 지도 학습 알고리즘**으로, **관측된 데이터를 통해 독립변수와 종속변수 사이의 숨어있는 관계를 추정하는 것**이다. 각 용어의 의미는 다음과 같다.<br>\n",
        "\n",
        "- **변수** : 변경될 수 있는 양이나 조건<br>\n",
        "- **독립변수** : 연구자가 임의로 조절할 수 있는 변수로, 실험 영역에 있어 다른 변수에 영향을 받지 않는 변수\n",
        "- **종속변수** : 관측이나 측정이 가능한 변수로, 독립변수에 영향을 받아서 변화하는 변수\n",
        "\n",
        "<br>\n",
        "\n",
        "특정 지역의 주택 면적과 최근 2년의 거래가격의 관계를 조사하는 경우, 다른 변수에 영향을 덜 받는 변수인 **주택의 면적은 독립변수**가 되고, **이에 영향을 받아 변화할 수 있는 거래가격이 종속변수**가 된다.<br>\n",
        "\n",
        "주택의 가격은 면적에도 영향을 받지만, 일조량 및 접근성 등에도 영향을 받을 수 있기 때문에, 이렇게 관측된 데이터를 바탕으로 **다차원 공간에 존재하는 데이터들을 가장 잘 설명하는 수학 함수를 찾는 것이 바로 회귀분석이 해야 할 일**이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "데이터에 숨겨진 관계를 표현하고 종속변수가 어떤 값을 가질지 예측하는 것을 **가설 혹은 모델**이라고 한다. 여러 개의 가설이 있을 때 **더 좋은 가설은 오차가 더 적은 가설**이다.<br>\n",
        "\n",
        "회귀 문제에서 추정한 함수가 만들어내는 오차를 측정하여 이 **오차를 줄이는 방향으로 함수의 계수를 최적화하는 과정을 수행한다면 (톰 미첼이 정의한) 기계학습**으로 볼 수 있다."
      ],
      "metadata": {
        "id": "6owIJY9D2Ilu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 4. 사이킷런을 이용한 선형 회귀\n",
        "\n",
        "<br><br>\n",
        "\n",
        "파이썬에서 사용할 수 있는 머신러닝 라이브러리들 중 가장 유명한 것이 **사이킷런**이다.<br>\n",
        "\n",
        "사이킷런은 2007년 구글 코드 프로젝트 모임의 몇몇 개발자들이 중심이 되어 시작되었다. 현재, 머신러닝을 위한 무료 라이브러리 배포라는 큰 목적을 위해 **scikit-learn.org라는 웹사이트를 통해 오픈소스 개발 방식으로 개발**이 이루어지고 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "머신러닝을 위해서는<br>\n",
        "1. 특성과 레이블(선택사항)로 이루어진 데이터\n",
        "2. 데이터를 바탕으로 동작이 결정되는 모델\n",
        "3. 모델을 위한 적절한 하이퍼파라미터\n",
        "4. 학습을 위한 훈련 단계\n",
        "5. 검증\n",
        "\n",
        "의 여러 단계가 필요하다.\n",
        "\n",
        "<br>\n",
        "\n",
        "사이킷런에서는 **지도 학습, 비지도 학습을 위한 다양한 모델을 제공**하며, 이 모델을 위한 **시각화 도구, 교차 검증 도구들까지 매우 광범위한 기능을 제공**한다.<br>\n",
        "\n",
        "단, 사이킷런에서는 심층 신경망, 합성곱 신경망, 순환 신경망 등과 같은 딥러닝 프레임워크는 제공하지 않으므로 이를 구현하기 위해서는 텐서플로, 파이토치와 같은 프레임워크를 사용하는 것이 좋다.\n",
        "\n",
        "<br>\n",
        "\n",
        "데이터를 학습시킬 때, 데이터를 원형 그대로 사용하는 경우도 있지만 일반적으로는 **데이터에서 특정 특성을 추출하여 학습시키고 테스트**하게 된다.<br>\n",
        "\n",
        "**특성이란, 관찰되는 현상에서 측정할 수 있는 개별적인 속성**을 의미한다. 그리고 기계에게 이 현상을 학습하게 한다는 것은 **이러한 특성을 입력으로 사용하여 학습**한다는 것이다.<br>\n",
        "\n",
        "y = f(x)의 함수를 찾는다 할 때, 입력 데이터로 사용되는 x가 바로 특성이다. **기계학습에서 특성이란 학습의 결과를 결정하는 데 영향을 미치는 입력 데이터**라고 할 수 있다.<br>\n",
        "\n",
        "ex)\n",
        "- 사람의 키와 몸무게, 개의 몸통 길이와 높이, 주택 가격과 주택의 면적 등"
      ],
      "metadata": {
        "id": "ddsHZ9iG4eNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 5. 선형 회귀 모델의 계수와 절편\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**선형 회귀는 임의의 독립 변수 x와 이 변수에 따른 종속 변수 y와의 상관관계를 모델링하는 기법**으로, 두 변수의 관계를 알아내거나 이 모델을 이용하여 y가 없는 x 값에 대해 y를 예측하는 데 사용할 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "가장 간단한 모델로, 이차원 평면 상에 있는 직선의 방정식은 기본적으로 다음과 같다.\n",
        "\n",
        "> ### y = mx + b\n",
        "\n",
        "여기에서 **m은 직선의 기울기**이고 **입력 변수 x에 곱해지는 계수**이다.<br>\n",
        "그리고, **x와 관계없이 y에 영향을 주는 값 b는 절편**이다. 절편은 y = mx라는 회귀선을 위 또는 아래로 얼마나 평행이동 시킬지를 결정한다.<br>\n",
        "\n",
        "기본적으로 **선형 회귀 알고리즘은 데이터를 설명하는 가장 적절한 기울기와 절편값을 찾는 것**이다. x변수는 데이터 특성이므로 변경할 수 없고, 우리가 **제어할 수 있는 값은 기울기와 절편**이다. 기울기와 절편의 값에 따라 여러 개의 직선이 있을 수 있다.<br>\n",
        "기본적으로 **선형 회귀 알고리즘은 데이터 요소에 여러 직선을 맞추어 본 후에 가장 적은 오류를 발생시키는 직선을 반환**한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "이 개념은 두 개 이상의 변수가 있는 경우까지 확장될 수 있다. 이를 **다중 회귀분석**이라고 한다.<br>\n",
        "\n",
        "이 경우 **종속 변수 g는 여러 독립 변수에 종속**된다.\n",
        "\n",
        "> ### ^y(w, x) = w0 + w1x1 + ... + wpxp\n",
        "\n",
        "여기에서 **w와 x는 모두 벡터**이고 **w0을 제외한 w = (w1, w2, ... wp)를 계수, w0를 절편**이라고도 한다. 이것은 사실 평면의 방정식이다.<br>\n",
        "\n",
        "2차원 공간에서 선형 회귀 모형은 직선이고, 3차원에서는 평면이고, 3차원 이상에서는 초평면이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "통계학에서는 보통 예측값을 나타낼 때 ^y(모자 쓴 y)와 같이 나타낸다.<br>\n",
        "\n",
        "선형 회귀분석을 위해서는 다음과 같은 **네 가지의 기본 가정**이 필요한다. 해당 가정들이 만족할 경우, 선형 회귀 모델의 예측력이 더욱 좋아질 것이다.\n",
        "\n",
        "- **선형성** : 독립변수와 종속변수 간의 분포 관계가 선형의 관계를 가진다.\n",
        "- **독립성** : 다중 회귀분석의 중요한 기본 가정으로, 독립변수와 다른 독립변수 간의 상관관계가 적을 경우 선형 회귀 모델의 예측력이 좋아진다.\n",
        "- **등분산성** : 분산이란 데이터의 분포 정도에 대한 척도인데, 데이터가 특정한 패턴 없이 고르게 분포하는 것이, 특정한 좁은 구간에만 집중해서 분포하는 것보다 더 나은 예측을 보인다.\n",
        "- **정규성** : 잔차란 회귀직선과 관측값과의 차이인데, 오차라고도 한다. 이 차이가 정규성을 만족해야 한다."
      ],
      "metadata": {
        "id": "wwTCEcZc9NPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 6. 간단한 선형 회귀의 수행\n",
        "\n",
        "<br><br>\n",
        "\n",
        "사이킷런을 코드에 가져오기 위해서는 **sklearn이라는 이름으로 import**하여 가져와야 한다."
      ],
      "metadata": {
        "id": "sY5fqAhiCBbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import linear_model  # 사이킷런 모듈 가져오기\n",
        "\n",
        "regr = linear_model.LinearRegression()"
      ],
      "metadata": {
        "id": "mFQ38IbGCQ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀를 구현하기 위해 **선형 모델 linear_model을 import**한 뒤에 **LinearRegression() 생성자를 통해 선형 회귀 모델을 생성**하였다. 참조하는 변수는 regr로 지정하였다.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "qwp6gaROCcMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[163], [179], [166], [169], [171]]\n",
        "y = [54, 63, 57, 56, 58]\n",
        "\n",
        "regr.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yxfL0_YCpDU",
        "outputId": "8b2e34db-21fb-41c1-bc9c-267b15511334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀를 위한 **입력 데이터 집합 X**를 만든다. **학습 데이터는 반드시 2차원 배열**이어야 한다. 사이킷런의 LinearRegession() 모델은 다중 회귀분석을 실시하기 위해서 설계되었기 때문이다. 이 때문에 X의 각 항목을 스칼라 값이 아닌, 다수의 독립 변수를 포함하는 벡터로 간주한다.<br>\n",
        "\n",
        "다음으로 정답에 해당하는** y 변수를 초기화**해준다.<br>\n",
        "\n",
        "그리고 **regr.fit(X, y)**와 같이 선형 회귀 모델에 입력과 출력을 지정한다.<br>\n",
        "선형 회귀분석을 적용하려면 **fit() 함수에 X와 y를 전달**한다. sklearn의 LinearRegression() 객체는 가능한 직선들 가운데 데이터를 가장 잘 따르는 직선을 계산해낼 수 있다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "sL4fzmmIC3Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from types import CodeType\n",
        "coef = regr.coef_  # 직선의 기울기\n",
        "intercept = regr.intercept_  # 직선의 절편\n",
        "score = regr.score(X, y)  # 학습된 직선이 데이터를 얼마나 잘 따르나\n",
        "\n",
        "print(\"y = {}* X + {:.2f}\".format(coef.round(2), intercept))\n",
        "print(\"데이터와 선형 회귀 직선의 관계점수 : {:.1%}\".format(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5lpvKvgD6ti",
        "outputId": "bd285d6b-e728-45db-a1ff-cbf06c74a515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = [0.53]* X + -32.50\n",
            "데이터와 선형 회귀 직선의 관계점수 : 91.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "직선의 기울기는 regr 모델의 coef_ 속성값으로 얻을 수 있으며, 직선의 절편은 intercept_ 속성값으로 얻을 수 있다.<br>\n",
        "사이킷런에서는 모델의 속성값의 가장 마지막에는 언더스코어(_) 문자를 넣어 표기하는데 위의 결과와 같이 coef_ 속성은 배열 형태를 하고 있음을 확인할 수 있다.<br>\n",
        "\n",
        "다음으로 score() 메소드를 통해 모델의 점수를 알아볼 수 있다.<br>\n",
        " sklearn의 선형 회귀 모델이 주어진 데이터를 따르는 직선을 생성하며 이 모델의 점수는 91.9점이라는 것을 보여주고 있다."
      ],
      "metadata": {
        "id": "x5lJU5NGEYOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 7. 데이터를 시각화하고 차원을 증가시키기\n",
        "\n",
        "<br><br>\n",
        "\n",
        "위에서 다뤄본 데이터를 시각화해본다면 보다 더 직관적으로 이해하기 좋을 것이다."
      ],
      "metadata": {
        "id": "NOnuw0EBE1sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X, y, color = 'blue', marker = 'D')  # 학습 데이터와 y 값을 산포도로 그리기\n",
        "y_pred = regr.predict(X)  # 학습 데이터를 입력으로 하여 예측값을 계산\n",
        "plt.plot(X, y_pred, 'r:')  # 계산된 기울기와 y 절편을 가지는 점선을 그리기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "3DgnNl-2rFvU",
        "outputId": "2306bffc-cb40-4cb7-a2e9-f417b1f6a960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7feda4620250>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파란색으로 표시된 **X와 y 데이터의 분포는 일정한 상관관계**를 가지며 선형 회귀 모델에 의해 계산된 직선이 빨간색 점선으로 표시되어 있다. 이 **직선은 현재 데이터의 분포를 가장 잘 설명하는 직선**이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "그래프와 같이 **데이터가 선형적인 형태로 일정한 상관관계를 가지며 분포**하는 것을 볼 수 있으며, regr이라는 모델이 X 값을 입력으로 받아서 예측한 ^y값(y_pred)이 이 데이터의 분포를 잘 설명하는 좋은 모델이라는 것도 확인해볼 수 있다.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "4KUs63C4ruX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unseen = [[167]]\n",
        "result = regr.predict(unseen)\n",
        "print('동윤이의 키가 {}cm이므로 몸무게는 {}kg으로 추정됨'.format(unseen, result.round(1)))"
      ],
      "metadata": {
        "id": "o8B9pCuQtEZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**predict() 메소드**를 사용하여 키를 입력하였을 때의 몸무게 추정값을 출력하였다.\n",
        "\n",
        "<br>\n",
        "\n",
        "이 모델은 **여자/남자를 하나의 선형방정식으로 표현하므로 부정확**하다. 여자/남자의 체중 차이를 반영한 선형 회귀 모델을 생성하자.<br>\n",
        "\n",
        "여자와 남자를 구분해야하므로 간단하게 남자는 0, 여자는 1로 **구분 값**을 넣는다."
      ],
      "metadata": {
        "id": "WalTtBWptwkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "X = [[168, 0], [166, 0], [173, 0], [163, 0], [177, 0], [163, 0], [178, 0], [172, 0], [163, 1], [162, 1], [171, 1], [162, 1], [164, 1], [162, 1], [158, 1], [173, 1], ]  # 2차원 입력 데이터\n",
        "y = [65, 61, 68, 63, 68, 61, 76, 67, 55, 51, 59, 53, 61, 56, 44, 57]\n",
        "\n",
        "regr.fit(X, y)  # 학습\n",
        "\n",
        "print(\"계수 :\", regr.coef_)\n",
        "print(\"절편 :\", regr.intercept_)\n",
        "print(\"점수 :\", regr.score(X, y))\n",
        "print(\"동윤이와 은지의 추정 몸무게 :\", regr.predict([[167, 0], [167, 1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvrMGG5auMfV",
        "outputId": "8c7bc7e4-1529-4e90-81c1-982859412b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "계수 : [ 0.71940741 -7.57833333]\n",
            "절편 : -56.17425925925929\n",
            "점수 : 0.8381612735811672\n",
            "동윤이와 은지의 추정 몸무게 : [63.96677778 56.38844444]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 8. 가설의 정확도를 평가하는 오차\n",
        "\n",
        "<br><br>\n",
        "\n",
        "어떤 데이터를 추정하는 가설이 얼마나 정확한지를 평가할 때, 그래프를 그려서 확인하는 것은 정확도를 시각적으로 표시할 수 있는 장점은 있지만, 서로 다른 가설을 비교할 때 정확한 척도로 사용하기에는 힘들다.\n",
        "\n",
        "<br>\n",
        "\n",
        "'더 나은' 선형방정식(혹은 '더 좋은' 가설)은 너무나 주관적인 표현이므로 이를 정량화하는 것이 필요하다. 이때 사용되는 것이 **오차함수**이다.<br>\n",
        "\n",
        "오차의 합을 그대로 사용하지 않고 별도의 오차함수를 사용하는 이유는, **오차가 있음에도 불구하고 예측모델의 오차를 0으로 처리하여 완벽한 모델로 간주하게 되는 경우**가 있을 수 있기 때문이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **평균 절대 오차** <br>\n",
        "\n",
        "머신러닝에서 사용 가능한 오차함수 중에서 **비교적 단순한 오차함수**로, 예측값 ^y와 관측값 y의 차이값의 절대값을 구한 후 이 값들의 평균값을 사용한다. 오차값을 그대로 보여주는 특징이 있다.<br>\n",
        "\n",
        "평균 절대 오차는 직관적이며 계산이 편리한 반면 문제점이 있다.<br>\n",
        "\n",
        "첫째, **축적을 보정하지 않기 때문에** 앞의 값의 10배에 해당하는 값에 대해서 동일한 10% 오차가 발생하더라도 10배의 **차이가 나는 문제점**이 있다.<br>\n",
        "\n",
        "둘째, **절대값의 사용으로 인해 미분이 불가능한 지점이 발생**한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **평균 제곱 오차** <br>\n",
        "\n",
        "**머신러닝에서 사용하는 대표적인 오차 척도**이다. 예측치 ^y와 정답 레이블 y 사이의 차이를 제곱하여 모두 더한 뒤에 전체 데이터의 개수 m으로 나누는 것이다.<br>\n",
        "\n",
        "파란점으로 표시된 레이블과 붉은 가설 직선의 y값은 차이가 난다. 이를 오차라고 하는데, **전체 오류(오차의 제곱==면적)의 합이 최소가 되는 모델이 가장 바람직한 모델**이며, 이 직선을 찾는 것이다."
      ],
      "metadata": {
        "id": "UDi3LrhrwOKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 9. 오차 함수의 구현과 파라미터 공간의 최적값\n",
        "\n",
        "<br><br>\n",
        "\n",
        "넘파이를 이용하여 오차 함수를 쉽게 구현할 수 있다."
      ],
      "metadata": {
        "id": "plxeiWKmy_u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  넘파이를 이용하여 구현한 평균 제곱 오차\n",
        "y = np.array([1.2, 2.4, 2.5, 4.6, 5.4])\n",
        "y_hat = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "diff = (y_hat - y) ** 2  # y_hat과 y의 차이값의 제곱\n",
        "e_mse = diff.sum() / len(y)\n",
        "print(\"평균 제곱 오차 :\", e_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h41BQBwz0YEW",
        "outputId": "6e3ba483-e2c8-45f8-a122-2748279c0455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평균 제곱 오차 : 0.19399999999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터가 y값에 저장되어 있고, 예측 모델이 y = 1x 꼴로 되어있어 x가 1에서 5까지 증가할 때, y_hat이 [1, 2, 3, 4, 5]인 경우를 가정하였다.\n",
        "\n",
        "<br>\n",
        "\n",
        "또한 동일한 기능을 **sklearn의 mean_squared_error() 함수**를 호출하여 실행시켜볼 수 있다."
      ],
      "metadata": {
        "id": "ftJIFqSU1GqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#  sklearn에서 제공하는 함수 사용하면 같은 결과가 나온다. 그럼 위에 왜 한겨?\n",
        "print(\"평균 제곱 오차 :\", mean_squared_error(y_hat, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RyOaVoK2Gk2",
        "outputId": "e4668329-e73c-450b-ba17-db5b83433ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평균 제곱 오차 : 0.19399999999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "오차를 제곱하는 데에는 더욱 중요한 이유가 있다. 이는 **오차 합 곡면의 기울기를 따라 내려가 최소 오차에 접근하기 위해서**이다.<br>\n",
        "\n",
        "양수 오차가 많으면 그 합이 무한히 커질 수도 있고, 음수 오차가 많으면 무한히 작은 값을 가질 수도 있다. 하지만, **오차를 제곱하면 가장 좋은 파라미터에서 최소값을 갖는 볼록한 그릇 모양의 곡면**을 만들 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**오차 곡선의 미분을 이용하여 곡선의 변화율을 구하고 이 변화율을 이용하여 최적해**를 찾을 수 있다. 오차 곡선에서 최소값(혹은 최적해)을 구하기 위하여 오차 곡선 혹은 곡면의 기울기를 따라 내려가며 해를 구하는 **경사 하강법**이라는 방법을 이용할 수 있다."
      ],
      "metadata": {
        "id": "915YCSFs2u9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "10. 미분과 경사 하강법\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**미분이란, 순간 변화량을 구하는 것**으로 독립변수 값의 변화량의 비의 극한으로 구성된다.<br>\n",
        "함수 f(x)의 1차 미분 f`(x)는 x가 매우 조금 변화한 정도에 대해 함수값이 어떤 비로 변화하는지 알려준다. 이를 **변화율**이라고 한다.<br>\n",
        "이러한 성질을 이용하면 머신러닝에서 매우 중요한 **최적화** 작업을 할 수 있다."
      ],
      "metadata": {
        "id": "zuOJBtCA3yS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적화와 관련된 용어들<br>\n",
        "\n",
        "- **목적함수** : (곡선) 변수 혹은 파라미터를 매개변수로 갖는 함수이다. 이를 f(x)라고 할 때, 이 함수를 가장 작은 값으로 만드는 최적의 변수 x*를 찾는 일을 최적화라고 한다. 위처럼 평균 제곱 오차의 최소값을 구할 경우, 평균 제곱 오차식이 목적함수가 될 것이다.\n",
        "\n",
        "- **평균 변화율** : (기울기를 나타내는 삼각형) 변수 x가 ∆x 만큼 변할 때, 목적함수는 ∆y 만큼 변한다. 이것의 비가 이 구간의 평균 변화율이다.\n",
        "\n",
        "- **미분과 접선의 기울기** : x = x^2인 지점에서 이 삼각형을 매우 작게 만들었다. 이 삼각형을 무한히 작게 만들면 그것이 x^2 지점에서의 목적함수 미분이다. 그리고 이 값은 그 지점에서 목적함수 곡선에 접하는 선, 즉 접선의 순간변화율이다. 이것은 접선의 기울기이므로 이 값의 크기는 경사가 급할수록 더 큰 값이 된다.\n",
        "\n",
        "- **경사 하강법** : 접선의 기울기를 따라 기울기 부호의 반대 방향으로 조금씩 내려오면 해당 지점에서 목적함수의 값이 더 작은 쪽으로 이동할 수 있다. x^2의 위치에서 x가 증가하면 y도 증가하므로 접선의 기울기는 양수이다. 즉, x^2에서 음의 방향으로 움직이면 목적함수의 값이 줄어든다. 즉, f`(x^2)의 반대 방향인 -f`(x^2)로 이동해야 한다. 이 과정을 반복하여 최소값을 구하는 방법을 경사 하강법이라고 한다. 반복적으로 조금씩 최소값에 접근하는 방법인데, 이 과정이 바로 학습의 과정이며 변화하는 변수 x의 **양의 학습률**이라고 한다. 최소값은 변화율이 0이 되는 지점으로, 변화율이란 곧 미분 값이다."
      ],
      "metadata": {
        "id": "4OsCY_8O4ZYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 7-1. 도전문제\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1. 다음 함수의 미분 함수 f`(x)를 구하여라.\n",
        "\n",
        "f(x) = 10           =>    f'(x) = 0  <br>\n",
        "f(x) = 3x + 2       =>    f'(x) = 3  <br>\n",
        "f(x) = 5x^2 + 2x    =>    f'(x) = 10x + 2  <br>\n",
        "f(x) = e^x          =>    f'(x) = e^x  <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. 정답값, 모델 A의 예측값, 모델 B의 예측값이 있다. 모델 A와 모델 B의 평균 절대 오차(MAE), 평균 제곱 오차(MSE)를 각각 구하여라."
      ],
      "metadata": {
        "id": "fYZ2YDw_6XJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "a = [[0.9], [1.3], [3.3], [3.8]]\n",
        "b = [[0.5], [1.9], [3.4], [4.4]]\n",
        "hat = np.array([1, 2, 3, 4])\n",
        "\n",
        "print(\"모델 A의 평균 절대 오차 :\", mean_absolute_error(hat, a))\n",
        "print(\"모델 B의 평균 절대 오차 :\", mean_absolute_error(hat, b))\n",
        "print(\"모델 A의 평균 제곱 오차 :\", mean_squared_error(hat, a))\n",
        "print(\"모델 B의 평균 제곱 오차 :\", mean_squared_error(hat, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZOiXGxB71es",
        "outputId": "d991d4b5-2dce-47a8-da48-d1873463c9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 A의 평균 절대 오차 : 0.32499999999999996\n",
            "모델 B의 평균 절대 오차 : 0.3500000000000001\n",
            "모델 A의 평균 제곱 오차 : 0.15749999999999997\n",
            "모델 B의 평균 제곱 오차 : 0.14500000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 11. 경사 하강법과 학습의 원리\n",
        "\n",
        "<br><br>\n",
        "\n",
        "경사 하강법을 구현하기 위해 **오차를 제곱하여 오차 곡면의 기울기를 따라 내려가 기울기기 0인 최소값을 찾는 과정**을 구현한다. 단순히 오차를 제곱하면 아래쪽이 볼록한 곡면이 되는 것을 이미 살펴보았는데, **직선의 기울기 w와 절편 b 각각에 대해 오차의 제곱을 구하게 되면 오차 곡면은 밥그릇 모양**을 띄게 될 것이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "직선의 기울기 w와 절편 b에 의해 결정되는 오차의 제곱 E^2(w, b)이 밥그릇 모양의 곡선이라면, 최적의 w와 b를 찾기 위한 오차 곡면의 기울기 방향은 오차의 제곱값을 기울기 w와 절편 b에 대해 각각 미분하여 생성된 벡터가 될 것이다.<br>\n",
        "\n",
        "이제 **E^2(w, b) 벡터를 -(Ex, E) 방향으로 조금 옮겨주면 최적의 w와 b에 가까워질 것**이다. (2Ex, 2E) 대신 -(Ex, E)를 사용한 이유는 두 벡터의 크기는 다르지만 방향이 같기 때문이며, 어차피 이 벡터에 학습률을 곱할 것이기 때문에 상수 2가 불필요하기 때문이다. 그리고 오차가 줄어드는 방향으로 이동해야 하므로 음수 부호가 필요하다.\n",
        "\n",
        "<br>\n",
        "\n",
        "이때 \"조금\"의 정도는 학습률을 의미하는 **learning_rate**라는 파라미터로 제어한다. 이렇게 훈련 모델을 구현하기 위하여 모델에 설정되는 학습에 사용되는 파라미터를 **하이퍼파라미터**라고 한다.<br>\n",
        "하이퍼파라미터에는 학습률, 훈련 반복 횟수, 가중치 초기화 값들이 될 수 있다.<br>\n",
        "\n",
        "기울기 w와 절편 b를 오차를 이용하여 수정할 때, 데이터별로 발생하는 오차를 하나씩 이용하여 갱신하는 것이 아니라, **벡터화 연산을 사용하여 직선의 기울기와 절편을 갱신**할 수 있다.<br>\n",
        "\n",
        "학습을 위해서는 전체 데이터를 모두 넣어서 에러를 구하는데, 이렇게 전체 데이터를 한 번 사용하는 것을 **1에폭**이라고 한다."
      ],
      "metadata": {
        "id": "0P_euU2j_7uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 4.5, 9, 10, 13])\n",
        "y = np.array([0, 0.2, 2.5, 5.4, 7.3])\n",
        "\n",
        "w, b = 0, 0  # 초기값 0\n",
        "learning_rate, epoch = 0.005, 1000  # 학습률과 학습 횟수(에폭)\n",
        "n = len(X)  # 입력데이터 개수\n",
        "\n",
        "for i in range(epoch):  # 학습 루프\n",
        "  y_pred = w * X + b  # 현재 w, b를 이용한 작업 T\n",
        "  error = y_pred - y  # 성능척도 P\n",
        "  w = w - learning_rate * (error * X).sum()  # 경험 E로 개선\n",
        "  b = b - learning_rate * error.sum()\n",
        "\n",
        "print(\"w =\", w.round(2), ', b =', b.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRQzVG6DGgPB",
        "outputId": "80444d78-efb1-4e7e-ae3e-ae2e205757c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.63 , b = -1.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1, 0), (4.5, 0.2), (9, 2.5), (10, 5.4), (13, 7.3)의 다섯 점들의 분포를 설명하는 선형 회귀 함수의 w와 b를 찾아보았다. 하이퍼파라미터인 w와 b의 초기값은 0이고 학습 횟수는 1000으로 설정하였다.<br>\n",
        "\n",
        "경사 하강법을 이용하여 구한 w와 b가 각각 0,63, -1.65인 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "f4OCyMwQHPP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 12. 경사 하강법과 학습률\n",
        "\n",
        "<br><br>\n",
        "\n",
        "위의 과정은 sklearn의 **LinearRegression 클래스**에 구현되어 있다."
      ],
      "metadata": {
        "id": "1EUlrR2UIRvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 4.5, 9, 10, 13])\n",
        "y = np.array([0, 0.2, 2.5, 5.4, 7.3])\n",
        "\n",
        "regr = linear_model.LinearRegression()  # 절편값 b는 0으로 둔다.\n",
        "X = X[:, np.newaxis]\n",
        "regr.fit(X, y)  # 학습\n",
        "\n",
        "print(\"w =\", regr.coef_.round(2), \", b =\", regr.intercept_.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-DGiamIdm0",
        "outputId": "643707df-7dc9-4917-b2de-a46d54823b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0.63] , b = -1.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "구한 가설 함수의 w와 b가 데이터의 분포를 제대로 설명하는가 시각화한다."
      ],
      "metadata": {
        "id": "CbJRYun8I1Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([1, 4.5, 9, 10, 13])\n",
        "y = np.array([0, 0.2, 2.5, 5.4, 7.3])\n",
        "\n",
        "plt.scatter(X, y, color = 'blue', marker = 'D')\n",
        "y_pred = 0.63 * X - 1.65  # 이전 계산에서 구한 x, b를 이용하여 선형 회귀 직선 그리기\n",
        "plt.plot(X, y_pred, 'r:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NFBzuQXFI9L8",
        "outputId": "09fd87e2-3813-41fa-bde9-fe4296adff6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa90dad78d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "만약 학습의 하이퍼파라미터 중 하나인 학습률을 매우 작은 0.00001로 둔다면 다음과 같은 결과가 나타난다."
      ],
      "metadata": {
        "id": "e3fT-S4TJpqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 4.5, 9, 10, 13])\n",
        "y = np.array([0, 0.2, 2.5, 5.4, 7.3])\n",
        "\n",
        "w, b = 0, 0  # 초기값 0\n",
        "learning_rate, eppch = 0.00001, 1000\n",
        "n = len(X)  # 입력데이터 개수\n",
        "\n",
        "for i in range(epoch):  # 학습 루프\n",
        "  y_pred = w * X + b  # 현재 w, b를 이용한 작업 T\n",
        "  error = y_pred - y  # 성능척도 P\n",
        "  w = w - learning_rate * (error * X).sum()  # 경험 E로 개선\n",
        "  b = b - learning_rate * error.sum()\n",
        "\n",
        "print(\"w =\", w.round(2), ', b =', b.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Sm0mL3JzFb",
        "outputId": "bffe2f0c-afb3-4621-bfdf-2ff69d04edd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.45 , b = 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "매 단계에서 사용해야 할 학습률이 너무 작을 경우 경사를 타고 내려오는 간격이 너무 작아 정답에 제대로 수렴하지 못하는 것을 볼 수 있다. 물론 학습 횟수를 매우 많이 준다면 언젠간 정답에 수렴할 수 있겠지만, 이 경우 학습에 너무 많은 시간이 걸릴 것이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "반대로 학습률을 1.0으로 둔 모습이다."
      ],
      "metadata": {
        "id": "V6_1VMeXKCGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 4.5, 9, 10, 13])\n",
        "y = np.array([0, 0.2, 2.5, 5.4, 7.3])\n",
        "\n",
        "w, b = 0, 0  # 초기값 0\n",
        "learning_rate, eppch = 1.0, 1000\n",
        "n = len(X)  # 입력데이터 개수\n",
        "\n",
        "for i in range(epoch):  # 학습 루프\n",
        "  y_pred = w * X + b  # 현재 w, b를 이용한 작업 T\n",
        "  error = y_pred - y  # 성능척도 P\n",
        "  w = w - learning_rate * (error * X).sum()  # 경험 E로 개선\n",
        "  b = b - learning_rate * error.sum()\n",
        "\n",
        "print(\"w =\", w.round(2), ', b =', b.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojtok_4HKToT",
        "outputId": "17d38857-bfe0-4ee3-ba25-78907b2423c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = nan , b = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사를 따라 이동하는 간격이 너무 커서 발산을 하게 되어 w와 b를 제대로 얻지 못하는 모습이다."
      ],
      "metadata": {
        "id": "jDnNeGj_KYKg"
      }
    }
  ]
}