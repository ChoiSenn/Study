!! 코랩에서 github 업로드 시, Unable to render code block 오류가 떴기에 우선 복사한 코드로 업로드. 추후 수정.



#3. 시맨틱 분할 (PSPNet)

<br><br>

## 3-1. 시맨틱 분할이란

<br>

시맨틱 분할 (semantic segmentation)
- 한 장의 화상에 포함된 여러 물체의 영역과 이름을 픽셀 수준에서 지정하는 작업.
- 물체 감지에서는 물체를 사각형의 BBox로 묶었지만, 시맨틱 분할에서는 픽셀 수준으로 '어디에서 어디까지 어떠한 클래스 물체인지' 라벨을 붙임.
- 제조업의 흠집 탐지, 의료 영상 진단의 병변 감지, 자율 운전의 주변 환경 파악 등에서 시맨틱 분할 기술을 사용.

<br>

시맨틱 분할의 입력 : 화상
시맨틱 분할의 출력 : 각 픽셀이 속한 클래스의 라벨 정보.

입력 화상의 크기가 세로 300px, 가로 500px, 분류할 물체의 클래스 스가 21종류라고 했을 때, 출력은 300x500 배열이며, 배열의 요소에는 물체의 클래스를 나타내는 인덱스 값 0~20 중 하나가 저장.

일반적으로 화상 데이터는 RGB 3요소의 배열로 표현되나, 시맨틱 분할의 출력은 요소가 하나이고 RGB 정보가 아닌 물체 클래스의 인덱스 값이 라벨 정보로 저장.

화상 표현 기법은 색상 정보를 표현하는 컬러 팔레트 형식을 취함. 0부터 순서대로 숫자에 RGB를 대응시킨 컬러 팔레트를 준비하고 해당 숫자(물체 라벨)와 RGB 값을 대응시켜 컬러 팔레트로 한 요소의 RGB를 표현할 수 있게 함.

<br>

시맨틱 분할 실습을 위해, PASCAL VOC2012 데이터 중 시맨틱 분할용 어노테이션이 포함된 화상 데이터만 사용. 훈련 데이터 1,464장, 검증 데이터 1,449장, 클래스 수는 총 21종류.

<br>

PSPNet(Pyramid Scene Parsing Network)을 활용한 물체 감지 흐름
- 1. 전처리로 화상 크기를 475x475 픽셀로 리사이즈하고 색상 정보를 표준화.
- 2. PSPNet 신경망에 전처리한 화상을 입력. 21x475x475(클래스 수, 높이, 폭)의 배열이 출력. 출력 배열의 값은 각 픽셀이 해당 클래스일 신뢰도에 대응한 값.
- 3. PSPNet 출력 값에 픽셀별로 신뢰도가 가장 높은 클래스와 각 픽셀이 대응될 것으로 예상되는 클래스를 구함. 픽셀별 신뢰도가 가장 높은 클래스 정보가 시맨틱 분할의 출력이 됨.
- 4. 시맨틱 분할의 출력을 입력 화상의 원 크기로 리사이즈.

<br><br>

## 3-2. 데이터셋과 데이터 로더 구현

<br>



import os
import urllib.request
import zipfile
import tarfile

# "data" 폴더가 존재하지 않는 경우 작성한다
data_dir = "./data/"
if not os.path.exists(data_dir):
    os.mkdir(data_dir)

# "weights" 폴더가 존재하지 않는 경우 작성한다
weights_dir = "./weights/"
if not os.path.exists(weights_dir):
    os.mkdir(weights_dir)

# VOC2012의 데이터 세트를 다운로드합니다
# 시간이 걸립니다(약 15분)
url = "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
target_path = os.path.join(data_dir, "VOCtrainval_11-May-2012.tar") 

if not os.path.exists(target_path):
    urllib.request.urlretrieve(url, target_path)
    
    tar = tarfile.TarFile(target_path)  # tar 파일 읽기
    tar.extractall(data_dir)  # tar 파일 압축 해제
    tar.close()  # tar 파일 닫기

<br>

먼저 화상 데이터 및 어노테이션 데이터 파일의 경로를 저장한 리스트를 만듦.

# 패키지 import
import os.path as osp
from PIL import Image

import torch.utils.data as data

def make_datapath_list(rootpath):
    """
    학습, 검증용 화상 데이터와 어노테이션 데이터의 파일 경로 리스트를 작성한다.

    Parameters
    ----------
    rootpath : str
        데이터 폴더의 경로

    Returns
    -------
    ret : train_img_list, train_anno_list, val_img_list, val_anno_list
        데이터의 경로를 저장한 리스트
    """

    # 화상 파일과 어노테이션 파일의 경로 템플릿을 작성
    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')
    annopath_template = osp.join(rootpath, 'SegmentationClass', '%s.png')

    # 훈련 및 검증 파일 각각의 ID(파일 이름)를 취득
    train_id_names = osp.join(rootpath + 'ImageSets/Segmentation/train.txt')
    val_id_names = osp.join(rootpath + 'ImageSets/Segmentation/val.txt')

    # 훈련 데이터의 화상 파일과 어노테이션 파일의 경로 리스트를 작성
    train_img_list = list()
    train_anno_list = list()

    for line in open(train_id_names):
        file_id = line.strip()  # 공백과 줄바꿈 제거
        img_path = (imgpath_template % file_id)  # 화상의 경로
        anno_path = (annopath_template % file_id)  # 어노테이션의 경로
        train_img_list.append(img_path)
        train_anno_list.append(anno_path)

    # 검증 데이터의 화상 파일과 어노테이션 파일의 경로 리스트 작성
    val_img_list = list()
    val_anno_list = list()

    for line in open(val_id_names):
        file_id = line.strip()  # 공백과 줄바꿈 제거
        img_path = (imgpath_template % file_id)  # 화상의 경로
        anno_path = (annopath_template % file_id)  # 어노테이션의 경로
        val_img_list.append(img_path)
        val_anno_list.append(anno_path)

    return train_img_list, train_anno_list, val_img_list, val_anno_list

# 동작 확인: 파일 경로 리스트를 취득
rootpath = "./data/VOCdevkit/VOC2012/"

train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(
    rootpath=rootpath)

print(train_img_list[0])
print(train_anno_list[0])

Dataset 클래스 작성 전에 화상과 어노테이션을 전처리하는 DataTransform 클래스를 만듦. 필요한 외부 클래스는 utils/data_augumentation.py 파일에서 구현.

먼저 화상 데이터와 어노테이션 데이터를 세트로 변환해야 함. Compose 클래스를 준비하고 Compose 내에서 데이터 변환 실시.

이어서 훈련 데이터의 데이터 확장. 화상 크기를 Scalse 클래스로 확대or축소하고, 크기가 커졌다면 원본 화상의 크기로 잘라내고, 작아졌다면 원본 화상 크기 만큼 검은색으로 채움. RandomRotation 클래스로 화상을 회전, RandomMirror 클래스에서 1/2 확률로 좌우 반전, Resize 클래스에서 지정된 화상 크기로 변환. 마지막으로 Normalize_Tensor 클래스를 활용하여 화상 데이터를 파이토치 텐서 형식으로 변환하고 색상 정보를 표준화.

검증 데이터는 데이터 확장을 실시하지 않고 Resize 클래스에 지정된 화상 크기로 변환한후 텐서 형식으로 Normalize_Tensor 클래스에서 변환 및 색상 정보 표준화만 적용함.

# 데이터 처리 클래스와 데이터 확장 클래스를 import
from utils.data_augumentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor

class DataTransform():
    """
    화상과 어노테이션의 전처리 클래스. 훈련시와 검증시 다르게 동작한다.
    화상의 크기를 input_size x input_size로 한다.
    훈련시에 데이터 확장을 수행한다.

    Attributes
    ----------
    input_size : int
        리사이즈 대상 화상의 크기.
    color_mean : (R, G, B)
        각 색상 채널의 평균값.
    color_std : (R, G, B)
        각 색상 채널의 표준편차.
    """

    def __init__(self, input_size, color_mean, color_std):
        self.data_transform = {
            'train': Compose([
                Scale(scale=[0.5, 1.5]),  # 화상의 확대
                RandomRotation(angle=[-10, 10]),  # 회전
                RandomMirror(),  # 랜덤 미러
                Resize(input_size),  # 리사이즈(input_size)
                Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서화
            ]),
            'val': Compose([
                Resize(input_size),  # 리사이즈(input_size)
                Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서화
            ])
        }

    def __call__(self, phase, img, anno_class_img):
        """
        Parameters
        ----------
        phase : 'train' or 'val'
            전처리 모드를 지정.
        """
        return self.data_transform[phase](img, anno_class_img)

Dataset 클래스인 VOCDataset 클래스를 작성. VOCDataset 인스턴스 생성 시 화상 데이터 리스트, 어노테이션 데이터 리스트, 학습인지 검증인지 나타내는 phase 변수, 그리고 전처리 클래스의 인스턴스를 인수로 받음.

화상을 읽어들이는 것은 OpenCV가 아닌 PIL을 사용하여, 색상 정보는 RGB 순서.

class VOCDataset(data.Dataset):
    """
    VOC2012의 Dataset을 만드는 클래스. PyTorch의 Dataset 클래스를 상속받는다.

    Attributes
    ----------
    img_list : 리스트
        어노테이션의 경로를 저장한 리스트
    anno_list : 리스트
        어노테이션의 경로를 저장한 리스트
    phase : 'train' or 'test'
        학습 또는 훈련을 설정한다.
    transform : object
        전처리 클래스의 인스턴스
    """

    def __init__(self, img_list, anno_list, phase, transform):
        self.img_list = img_list
        self.anno_list = anno_list
        self.phase = phase
        self.transform = transform

    def __len__(self):
        '''화상의 매수를 반환'''
        return len(self.img_list)

    def __getitem__(self, index):
        '''
        전처리한 화상의 텐서 형식 데이터와 어노테이션을 취득
        '''
        img, anno_class_img = self.pull_item(index)
        return img, anno_class_img

    def pull_item(self, index):
        '''화상의 텐서 형식 데이터, 어노테이션을 취득한다'''

        # 1. 화상 읽기
        image_file_path = self.img_list[index]
        img = Image.open(image_file_path)   # [높이][폭][색RGB]

        # 2. 어노테이션 화상 읽기
        anno_file_path = self.anno_list[index]
        anno_class_img = Image.open(anno_file_path)   # [높이][폭]

        # 3. 전처리 실시
        img, anno_class_img = self.transform(self.phase, img, anno_class_img)

        return img, anno_class_img

정상적으로 데이터셋의 인스턴스가 만들어져 데이터를 꺼낼 수 있는지 확인.

# 동작 확인

# (RGB) 색의 평균치와 표준편차
color_mean = (0.485, 0.456, 0.406)
color_std = (0.229, 0.224, 0.225)

# 데이터 세트 작성
train_dataset = VOCDataset(train_img_list, train_anno_list, phase="train", transform=DataTransform(
    input_size=475, color_mean=color_mean, color_std=color_std))

val_dataset = VOCDataset(val_img_list, val_anno_list, phase="val", transform=DataTransform(
    input_size=475, color_mean=color_mean, color_std=color_std))

# 데이터를 추출하는 예
print(val_dataset.__getitem__(0)[0].shape)
print(val_dataset.__getitem__(0)[1].shape)
print(val_dataset.__getitem__(0))


마지막으로 DataLoader를 만듦. 어노테이션 데이터 크기가 데이터마다 변하지 않아 파이토치의 DataLoader 클래스를 그대로 사용할 수 있음.

훈련 및 검증 데이터 각각의 DataLoader를 작성하여 사전형 변수로 정리.

# 데이터 로더 작성
batch_size = 8

train_dataloader = data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True)

val_dataloader = data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False)

# 사전 오브젝트로 정리한다
dataloaders_dict = {"train": train_dataloader, "val": val_dataloader}

# 동작 확인
batch_iterator = iter(dataloaders_dict["val"])  # 반복자로 변환
imges, anno_class_imges = next(batch_iterator)  # 첫번째 요소를 꺼낸다
print(imges.size())  # torch.Size([8, 3, 475, 475])
print(anno_class_imges.size())  # torch.Size([8, 3, 475, 475])

<br><br>

## 3-3. PSPNet 네트워크 구성 및 구현

<br>

PSPNet은 Feature, Pyramid Pooling, Decoder, AuxLoss라는 네 개의 모듈로 구성.

Feature 모듈
- Encoder 모듈로도 불림.
- 입력 화상의 특징을 파악하는 것이 목적. 
- 출력은 2048x60x60(chx높이x폭).
- 화상의 특징을 파악하는 채널을 2,048개 준비하는 것, 특징량의 화상 크기는 처음보다 작은 60x60픽셀이 된다는 점이 중요.

Pyramid Pooling 모듈
- 이 모듈로 해결하려는 문제는 '어떠한 픽셀의 물체 라벨을 구하려면 다양한 크기로 해당 픽셀 주변 정보가 필요'.
- 즉, 어떠한 픽셀의 물체 라벨을 구하려면 그 픽셀이나 주변의 정보 뿐만 아니라 더 넓은 범위의 화상 정보가 필요.
- 네 가지 크기의 특징량 맵을 준비. 화상 전체를 차지하는 특징량, 화상 절반을 차지하는 특징량, 화상의 1/3을 차지하는 특징량, 화상의 1/6을 차지하는 특징량으로 Feature 모듈의 출력을 처리.
- 출력 데이터 크기는 4096x60x60(xhx높이x폭).

Decoder 모듈
- 업샘플링 모듈로도 불림.
- 첫 번째 목적은 Pyramid Pooling 모듈의 출력을 21x60x60(클래스수x높이x폭) 텐서로 변환하는 것. 출력 데이터 값은 각 픽셀이 전체 21 클래스에 각기 속할 확률값.
- 두 번째 목적은 21x60x60(클래스수x높이x폭)으로 변환된 텐서를 원 입력 화상 크기에 맞도록 21x475x475로 변환하는 것. 화상 크기가 줄어든 텐서를 원래 크기로 되돌림. 출력은 21x475x475.
- 추론 시, Decoder 모듈의 출력으로 출력에 대한 최대 확률의 물체 클래스를 찾아 각 픽셀의 라벨을 결정.

AuxLoss 모듈
- 원래는 세 개의 모듈로 시맨틱 분할이 이루어지지만, PSPNet에서는 네트워크 결합 파라미터의 학습을 더 잘 수행하기 위해 AuxLoss 모듈을 준비.
- 손실함수 계산을 보조. Feature 모듈로 중간 텐서를 빼내 입력 데이터로 함. 이를 Decoder 모듈처럼 각 픽셀에 대응한느 물체 라벨 추정 클래스 분류를 실행.
- 신경망 학습 시에는 AuxLoss 모듈의 출력과 Decoder 모듈의 출력을 모두 화상의 어노테이션 데이터로 대응시켜 손실 값을 계산. 계산 후 손실 값에 따른 오차 역전파 법을 실시하여 네트워크의 결합 파라미터를 갱신.
- AuxLoss 모듈은 Feature 층의 중간까지 결과로 시맨틱 분할을 실시하여 분류 정확도는 떨어짐. 그러나 오차 역전파법 수행 시 Feature 층 중간까지의 네트워크 파라미터가 더 좋은 값이 되도록 도울 수 있음.
- 학습 시에는 사용하지만 추론 시에는 해당 모율의 출력은 사용하지 않고 Decoder 모듈의 출력만으로 시맨틱 분할.

<br>

네 개의 모듈로 구성된 PSPNet 클래스를 구현. 

생성자에서는 우선 PSPNet 형태를 규정하는 파라미터를 설정. 인수로서 클래스 수만 취하고 나머지는 하드 코딩.

각 모듈의 오브젝트 준비. Feature 모듈은 다섯 개의 서브 네트워크로, 기타 모듈은 각각 하나의 서브 네트워크로 구성.

PSPNet 클래스의 메소드는 forward 뿐임. 순서대로 각 모듈의 서브 네트워크를 실행.

정리하자면, PSPNet은 네 개의 모듈을 준비하고 forward 메서드에서 이를 순전파함.

# 패키지 import
import torch
import torch.nn as nn
import torch.nn.functional as F

class PSPNet(nn.Module):
    def __init__(self, n_classes):
        super(PSPNet, self).__init__()

        # 파라미터 설정
        block_config = [3, 4, 6, 3]  # resnet50
        img_size = 475
        img_size_8 = 60  # img_size의 1/8로 설정

        # 4개의 모듈을 구성하는 서브 네트워크 준비
        self.feature_conv = FeatureMap_convolution()
        self.feature_res_1 = ResidualBlockPSP(
            n_blocks=block_config[0], in_channels=128, mid_channels=64, out_channels=256, stride=1, dilation=1)
        self.feature_res_2 = ResidualBlockPSP(
            n_blocks=block_config[1], in_channels=256, mid_channels=128, out_channels=512, stride=2, dilation=1)
        self.feature_dilated_res_1 = ResidualBlockPSP(
            n_blocks=block_config[2], in_channels=512, mid_channels=256, out_channels=1024, stride=1, dilation=2)
        self.feature_dilated_res_2 = ResidualBlockPSP(
            n_blocks=block_config[3], in_channels=1024, mid_channels=512, out_channels=2048, stride=1, dilation=4)

        self.pyramid_pooling = PyramidPooling(in_channels=2048, pool_sizes=[
            6, 3, 2, 1], height=img_size_8, width=img_size_8)

        self.decode_feature = DecodePSPFeature(
            height=img_size, width=img_size, n_classes=n_classes)

        self.aux = AuxiliaryPSPlayers(
            in_channels=1024, height=img_size, width=img_size, n_classes=n_classes)

    def forward(self, x):
        x = self.feature_conv(x)
        x = self.feature_res_1(x)
        x = self.feature_res_2(x)
        x = self.feature_dilated_res_1(x)

        output_aux = self.aux(x)  # Feature 모듈의 중간을 Aux 모듈로

        x = self.feature_dilated_res_2(x)

        x = self.pyramid_pooling(x)
        output = self.decode_feature(x)

        return (output, output_aux)

<br><br>

## 3-4. Feature 모듈 구현

<br>

Feature 모듈을 구현한다.

Feature 모듈은 다섯 개의 서브 네트워크인 FeatureMap_convolution, 두 개의 ResidualBlockPSP, 두 개의 dilated (확장)판 ResidualBlockPSP로 구성.

<br>

FeatureMap_convolution
- Feature 모듈을 구성하는 최초 서브 네트워크. 
- FeatureMap_convolution으로의 입력은 전처리된 화상이며 3x475x475 크기. 이 텐서는 FeatureMap_convolution 출력 시 128x119x119로 바뀜. 
- 합성곱 층, 배치 정규화, ReLU를 세트로 하는 conv2dBatchNormRelu가 세 개 존재하며 최대 풀링 층이 있음.
- 단순히 합성곱, 배치 정규화, 최대 풀링으로 화상의 특징량을 추출하는 역할을 함.

<br>

서브 네트워크 FeatureMap_convolution를 구현.

먼저 합성곱 층과 배치 정규화, ReLU를 세트로 하는 conv2dBatchNormRelu 클래스를 만듦.

class conv2DBatchNormRelu(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):
        super(conv2DBatchNormRelu, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels,
                              kernel_size, stride, padding, dilation, bias=bias)
        self.batchnorm = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        # inplase 설정으로, 입력을 저장하지 않고 출력을 계산하여 메모리 절약

    def forward(self, x):
        x = self.conv(x)
        x = self.batchnorm(x)
        outputs = self.relu(x)

        return outputs

conv2dBatchNormRelu 클래스를 사용하여 FeatureMap_convolution 클래스를 만듦.

생성자에서 conv2dBatchNormRelu 세 개와 최대 풀링 층을 준비하여 이 네 개를 순전파하는 forward 메서드를 정의.

class FeatureMap_convolution(nn.Module):
    def __init__(self):
        '''구성할 네트워크 준비'''
        super(FeatureMap_convolution, self).__init__()

        # 합성곱 층1
        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 3, 64, 3, 2, 1, 1, False
        self.cbnr_1 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)

        # 합성곱 층2
        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 64, 3, 1, 1, 1, False
        self.cbnr_2 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)

        # 합성곱 층3
        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 128, 3, 1, 1, 1, False
        self.cbnr_3 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)

        # MaxPooling 층
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        x = self.cbnr_1(x)
        x = self.cbnr_2(x)
        x = self.cbnr_3(x)
        outputs = self.maxpool(x)
        return outputs

Feature 모듈을 구성하는 두 개의 ResidualBlockPSP, 그리고 두 개의 dilated 판 ResidualBlockPSP를 구현. 서브 네트워크 ResidualBlockPSP는 ResidualNetwork(ResNet) 신경망에서 사용되는 ResidualBlock 구조를 사용.

ResidualBlockPSP는 bottleNeckPSP 클래스를 지나 bottleNeckIdentifyPSP 클래스를 여러 번 반복하여 출력. Feature 모듈 네 개의 ResidualBlockPSP 서브 네트워크에서 bottleNeckIdentifyPSP 클래스가 반복되는 횟수는 각각 3, 4, 6, 3회이며 이는 변할 수 있음.

서브 네트워크 ResidualBlockPSP를 구현. 생성자에서 bottleNeckPSP 한 개와 bottleNeckIdentifyPSP 여러 개를 준비.

class ResidualBlockPSP(nn.Sequential):
    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):
        super(ResidualBlockPSP, self).__init__()

        # bottleNeckPSP를 준비
        self.add_module(
            "block1",
            bottleNeckPSP(in_channels, mid_channels,
                          out_channels, stride, dilation)
        )

        # bottleNeckIdentifyPSP 반복 준비
        for i in range(n_blocks - 1):
            self.add_module(
                "block" + str(i+2),
                bottleNeckIdentifyPSP(
                    out_channels, mid_channels, stride, dilation)
            )


ResidualBlockPSP 클래스에서 사용되는 bottleNeckPSP 클래스와 bottleNeckIdentifyPSP 클래스를 구현.

bottleNeckPSP와 bottleNeckIdentifyPSP의 차이는 스킬 결합에 합성곱 층이 들어가는지의 여부. bottleNeckPSP는 합성곱 층을 한 번 적용하지만 bottleNeckIdentifyPSP는 적용하지 않음.

먼저 합성곱 층과 배치 정규화만 있는 conv2DBatchNorm 클래스를 구현한 후, bottleNeckPSP와 bottleNeckIdentifyPSP를 구현.

class conv2DBatchNorm(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):
        super(conv2DBatchNorm, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels,
                              kernel_size, stride, padding, dilation, bias=bias)
        self.batchnorm = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.conv(x)
        outputs = self.batchnorm(x)

        return outputs

class bottleNeckPSP(nn.Module):
    def __init__(self, in_channels, mid_channels, out_channels, stride, dilation):
        super(bottleNeckPSP, self).__init__()

        self.cbr_1 = conv2DBatchNormRelu(
            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)
        self.cbr_2 = conv2DBatchNormRelu(
            mid_channels, mid_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)
        self.cb_3 = conv2DBatchNorm(
            mid_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

        # 스킵 결합
        self.cb_residual = conv2DBatchNorm(
            in_channels, out_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))
        residual = self.cb_residual(x)
        return self.relu(conv + residual)

class bottleNeckIdentifyPSP(nn.Module):
    def __init__(self, in_channels, mid_channels, stride, dilation):
        super(bottleNeckIdentifyPSP, self).__init__()

        self.cbr_1 = conv2DBatchNormRelu(
            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)
        self.cbr_2 = conv2DBatchNormRelu(
            mid_channels, mid_channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)
        self.cb_3 = conv2DBatchNorm(
            mid_channels, in_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))
        residual = x
        return self.relu(conv + residual)


<br><br>

## 3-5. Pyramid Pooling 모듈 구현

<br>

Pyramid Pooling 모듈은 PyramidPooling 클래스로 이루어진 하나의 서브 네트워크로 구성.

Pyramid Pooling 모듈의 입력은 Feature 모듈에서 출력된 크기 2048x60x60의 텐서. 출력 텐서는 Pyramid Pooling으로 멀티 스테일 정보를 가짐. 멀티 스케일 정보를 가진 Pyramid Pooling 모듈의 출력 텐서는 각 픽셀의 클래스를 정할 때 해당 픽셀 주변에 있는 다양한 스케일의 특징량 정보를 사용할 수 있어 높은 정밀도로 시맨틱 분할 실현 가능.

<br>

Pyramid Pooling 모듈은 PyramidPooling 클래스만으로 구축. PyramidPooling은 입력을 다섯 개로 분기시킨 후 Adaptive Average Pooling 층, conv2DBatchNormRelu 클래스, UpSample 층을 통과시켜 마지막에 하나의 텐서로 재결합.



class PyramidPooling(nn.Module):
    def __init__(self, in_channels, pool_sizes, height, width):
        super(PyramidPooling, self).__init__()

        # forward에 사용하는 화상 크기
        self.height = height
        self.width = width

        # 각 합성곱 층의 출력 채널 수
        out_channels = int(in_channels / len(pool_sizes))

        # 각 합성곱 층을 작성
        # 다음은 for문으로 구현하는 것이 낫지만, 이해를 돕기 위해 하나하나 나열하고 있습니다
        # pool_sizes: [6, 3, 2, 1]
        self.avpool_1 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[0])
        self.cbr_1 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

        self.avpool_2 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[1])
        self.cbr_2 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

        self.avpool_3 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[2])
        self.cbr_3 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

        self.avpool_4 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[3])
        self.cbr_4 = conv2DBatchNormRelu(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

    def forward(self, x):

        out1 = self.cbr_1(self.avpool_1(x))
        out1 = F.interpolate(out1, size=(
            self.height, self.width), mode="bilinear", align_corners=True)

        out2 = self.cbr_2(self.avpool_2(x))
        out2 = F.interpolate(out2, size=(
            self.height, self.width), mode="bilinear", align_corners=True)

        out3 = self.cbr_3(self.avpool_3(x))
        out3 = F.interpolate(out3, size=(
            self.height, self.width), mode="bilinear", align_corners=True)

        out4 = self.cbr_4(self.avpool_4(x))
        out4 = F.interpolate(out4, size=(
            self.height, self.width), mode="bilinear", align_corners=True)

        # 최종적으로 결합시킬, dim=1으로 채널 수의 차원으로 결합
        output = torch.cat([x, out1, out2, out3, out4], dim=1)

        return output

<br><br>

## 3-6. Decoder, AuxLoss 모듈 구현

<br>

Decoder 및 AuxLoss 모듈은 Pyramid Pooling 또는 Feature 모듈에서 출력된 텐서 정보를 Decode(읽기) 함.  텐서 정보를 읽은 후, 픽셀 별로 물체 라벨을 클래스 분류로 추정하고 마지막으로 화상 크기를 원래의 475x475로 업샘플링함.

Decoder 모듈은 DecodePSPFeature 클래스, AuxLoss 모듈은 AuxiliaryPSPlayers 클래스로 구현. Pyramid Pooling 모듈과 마찬가지로 화상의 확대 처리인 UpSample을 F.interpolate 연산으로 실행.

class DecodePSPFeature(nn.Module):
    def __init__(self, height, width, n_classes):
        super(DecodePSPFeature, self).__init__()

        # forward에 사용하는 화상 크기
        self.height = height
        self.width = width

        self.cbr = conv2DBatchNormRelu(
            in_channels=4096, out_channels=512, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)
        self.dropout = nn.Dropout2d(p=0.1)
        self.classification = nn.Conv2d(
            in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        x = self.cbr(x)
        x = self.dropout(x)
        x = self.classification(x)
        output = F.interpolate(
            x, size=(self.height, self.width), mode="bilinear", align_corners=True)

        return output

class AuxiliaryPSPlayers(nn.Module):
    def __init__(self, in_channels, height, width, n_classes):
        super(AuxiliaryPSPlayers, self).__init__()

        # forward에 사용하는 화상 크기
        self.height = height
        self.width = width

        self.cbr = conv2DBatchNormRelu(
            in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)
        self.dropout = nn.Dropout2d(p=0.1)
        self.classification = nn.Conv2d(
            in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        x = self.cbr(x)
        x = self.dropout(x)
        x = self.classification(x)
        output = F.interpolate(
            x, size=(self.height, self.width), mode="bilinear", align_corners=True)

        return output

마지막 클래스 분류(self.classification) 시, 전결합 층을 사용하지 않고 클래스 수와 동일하네 21을 출력 채널로 하는 커널 크기 1의 합성곱 층을 사용하는 것이 특징. 커널 크기 1의 합성곱 층은 점별 합성곱이라는 특수한 기법.

PSPNet 네트워크 구조, 네트워크의 순전파 계산을 모두 구현하였으니 마지막으로 네트워크 모델인 PSPNet 클래스의 인스턴스를 작성하고 오류 없이 계산된느지 확인.

# 모델 정의
net = PSPNet(n_classes=21)
net

# 더미 데이터 작성
batch_size = 2
dummy_img = torch.rand(batch_size, 3, 475, 475)

# 계산
outputs = net(dummy_img)
print(outputs)


<br><br>

## 3-7. 파인튜닝을 활용한 학습 및 검증

<br>

학습된 모델로 파인튜닝을 실행. 

먼저 데이터 로더를 만듦. 미니 배치 크기는 1GPU 메모리에 담기는 여덟 개로 설정. 입력 화상 크기가 475x475이므로 미니 배치 사이즈를 크게 하면 한 개의 GPU 메모리에 담기지 않음.

# 패키지 import
import random
import math
import time
import pandas as pd
import numpy as np

import torch
import torch.utils.data as data
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
import torch.optim as optim

# 초기설정
# Setup seeds
torch.manual_seed(1234)
np.random.seed(1234)
random.seed(1234)

from utils.dataloader import make_datapath_list, DataTransform, VOCDataset

# 파일 경로 리스트 작성
rootpath = "./data/VOCdevkit/VOC2012/"
train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(
    rootpath=rootpath)

# Dataset 작성
# (RGB) 색의 평균값과 표준편차
color_mean = (0.485, 0.456, 0.406)
color_std = (0.229, 0.224, 0.225)

train_dataset = VOCDataset(train_img_list, train_anno_list, phase="train", transform=DataTransform(
    input_size=475, color_mean=color_mean, color_std=color_std))

val_dataset = VOCDataset(val_img_list, val_anno_list, phase="val", transform=DataTransform(
    input_size=475, color_mean=color_mean, color_std=color_std))

# DataLoader 작성
batch_size = 8

train_dataloader = data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True)

val_dataloader = data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False)

# 사전형 변수로 정리
dataloaders_dict = {"train": train_dataloader, "val": val_dataloader}


네트워크 모델을 만들기 위해 먼저 ADE20K 네트워크 모델을 준비. 출력 클래스 수는 ADE20K 데이터셋에 맞춰 150. 학습된 파라미터 .pth를 읽고, 최종 출력층을 Pascal VOC의 21 클래스로 하기 위해 Decoder와 AuxLoss 모듈의 분류용 합성곱 층을 바꾸면 21 클래스에 대응한 PSPNet이 됨.

교체한 합성곱 층은 Xavier의 초깃값으로 초기화함.

이번에는 ReLU의 He 초깃값 대신 분류용 유닛의 마지막 층이자 활성화 함수인 시그모이드 함수를 사용. 활성화 함수가 시그모이드 함수일 경우, Xavier 초깃값으로 초기화.

from utils.pspnet import PSPNet

# 파인 튜닝으로 PSPNet을 작성
# ADE20K 데이터 세트의 학습된 모델을 사용하며, ADE20K는 클래스 수가 150입니다
net = PSPNet(n_classes=150)

# ADE20K 학습된 파라미터를 읽기
state_dict = torch.load("./weights/pspnet50_ADE20K.pth")
net.load_state_dict(state_dict)

# 분류용의 합성곱층을, 출력수 21으로 바꿈
n_classes = 21
net.decode_feature.classification = nn.Conv2d(
    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)

net.aux.classification = nn.Conv2d(
    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)

# 교체한 합성곱층을 초기화한다. 활성화 함수는 시그모이드 함수이므로 Xavier를 사용한다.
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        nn.init.xavier_normal_(m.weight.data)
        if m.bias is not None:  # 바이어스 항이 있는 경우
            nn.init.constant_(m.bias, 0.0)

net.decode_feature.classification.apply(weights_init)
net.aux.classification.apply(weights_init)

print('네트워크 설정 완료: 학습된 가중치를 로드했습니다')

다 클래스 분류의 손실함수인 크로스 엔트로피 오차 함수로 손실함수를 구현. 메인 손실과 AuxLoss 손실 합을 총 손실로 함. AuxLoss는 계수 0.4를 곱하여 가중치를 메인 손실보다 작게 함.

# 손실함수 정의
class PSPLoss(nn.Module):
    """PSPNet의 손실함수 클래스입니다"""

    def __init__(self, aux_weight=0.4):
        super(PSPLoss, self).__init__()
        self.aux_weight = aux_weight  # aux_loss의 가중치

    def forward(self, outputs, targets):
        """
        손실함수 계산

        Parameters
        ----------
        outputs : PSPNet의 출력(tuple)
            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。

        targets : [num_batch, 475, 475]
            정답 어노테이션 정보

        Returns
        -------
        loss : 텐서
            손실값
        """

        loss = F.cross_entropy(outputs[0], targets, reduction='mean')
        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')

        return loss+self.aux_weight*loss_aux


criterion = PSPLoss(aux_weight=0.4)

마지막으로 파라미터 최적화 기법을 정의함. 파인튜닝이기 때문에 입력에 가까운 모듈의 학습률은 작게, 교체한 합성곱 층을 가진 Decoder와 AuxLoss 모듈은 크게 설정.

이번에는 에폭에 따라 학습률을 변화시키는 스케쥴러를 사용.

# 파인 튜닝이므로, 학습률은 작게
optimizer = optim.SGD([
    {'params': net.feature_conv.parameters(), 'lr': 1e-3},
    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},
    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},
    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},
    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},
    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},
    {'params': net.decode_feature.parameters(), 'lr': 1e-2},
    {'params': net.aux.parameters(), 'lr': 1e-2},
], momentum=0.9, weight_decay=0.0001)


# 스케쥴러 설정
def lambda_epoch(epoch):
    max_epoch = 30
    return math.pow((1-epoch/max_epoch), 0.9)


scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)

마지막으로 학습 및 검증용 합수인 train_model을 구현. 스케줄러를 사용하고, 멀티플 미니 배치를 사용한다는 점이 달라짐.

batch_multiplier = 3으로 정의하고 3회에 한 번 optimizer.step()을 실행하여 미니 배치 크기를 24개로 함.

# 모델을 학습시키는 함수를 작성
def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):

    # GPU가 사용 가능한지 확인
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("사용 장치: ", device)

    # 네트워크를 GPU로
    net.to(device)

    # 네트워크가 어느 정도 고정되면 고속화한다
    torch.backends.cudnn.benchmark = True

    # 화상의 매수
    num_train_imgs = len(dataloaders_dict["train"].dataset)
    num_val_imgs = len(dataloaders_dict["val"].dataset)
    batch_size = dataloaders_dict["train"].batch_size

    # 반복자의 카운터 설정
    iteration = 1
    logs = []

    # multiple minibatch
    batch_multiplier = 3

    # epoch 루프
    for epoch in range(num_epochs):

        # 시작 시간 저장
        t_epoch_start = time.time()
        t_iter_start = time.time()
        epoch_train_loss = 0.0  # epoch의 손실합
        epoch_val_loss = 0.0  # epoch의 손실합

        print('-------------')
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-------------')

        # epoch별 훈련 및 검증 루프
        for phase in ['train', 'val']:
            if phase == 'train':
                net.train()  # 모델을 훈련 모드로
                scheduler.step()  # 최적화 scheduler 갱신
                optimizer.zero_grad()
                print('(train)')

            else:
                if((epoch+1) % 5 == 0):
                    net.eval()   # 모델을 검증 모드로
                    print('-------------')
                    print('(val)')
                else:
                    # 검증은 다섯 번 중에 한 번만 수행
                    continue

            # 데이터 로더에서 minibatch씩 꺼내 루프
            count = 0  # multiple minibatch
            for imges, anno_class_imges in dataloaders_dict[phase]:
                # 미니배치 크기가 1이면 배치 노멀라이제이션에서 오류가 발생하므로 회피
                if imges.size()[0] == 1:
                    continue

                # GPU가 사용가능하면 GPU에 데이터를 보낸다
                imges = imges.to(device)
                anno_class_imges = anno_class_imges.to(device)

                # multiple minibatch로 파라미터 갱신
                if (phase == 'train') and (count == 0):
                    optimizer.step()
                    optimizer.zero_grad()
                    count = batch_multiplier

                # 순전파(forward) 계산
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = net(imges)
                    loss = criterion(
                        outputs, anno_class_imges.long()) / batch_multiplier

                    # 훈련시에는 역전파
                    if phase == 'train':
                        loss.backward()  # 경사 계산
                        count -= 1  # multiple minibatch

                        if (iteration % 10 == 0):  # 10iter에 한 번, loss를 표시
                            t_iter_finish = time.time()
                            duration = t_iter_finish - t_iter_start
                            print('반복 {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(
                                iteration, loss.item()/batch_size*batch_multiplier, duration))
                            t_iter_start = time.time()

                        epoch_train_loss += loss.item() * batch_multiplier
                        iteration += 1

                    # 검증 시
                    else:
                        epoch_val_loss += loss.item() * batch_multiplier

        # epoch의 phase별 loss와 정답률
        t_epoch_finish = time.time()
        print('-------------')
        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(
            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))
        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))
        t_epoch_start = time.time()

        # 로그 저장
        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /
                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}
        logs.append(log_epoch)
        df = pd.DataFrame(logs)
        df.to_csv("log_output.csv")

    # 최후의 네트워크를 저장
    torch.save(net.state_dict(), 'weights/pspnet50_' +
               str(epoch+1) + '.pth')

마지막으로 학습 및 검증을 수행.  (실행 확인 후 중단함.)

# 학습 및 검증 실행
num_epochs = 30  # 12시간 걸린다고 함. 헐~
train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)

<br><br>

## 3-8. 시맨틱 분할 추론

<br>

앞서 학습시킨 모델을 사용.

승마 화상에 대해 추론하며, 어노테이션 화상을 한 장 사용.

어노테이션 화상을 사용하는 이유
- 없으면 전처리 클래스의 함수가 제대로 작동하지 않게 됨. 실제로 추론에 사용하지 않지만 더미 데이터로서 함수에 전달.
- 어노테이션 화상에서 색상 팔레트 정보를 추출하지 않으면 물체 라벨에 해당하는 색상 정보가 존재하지 않게 됨.

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import torch

from utils.dataloader import make_datapath_list, DataTransform

# 파일 경로 리스트 작성
rootpath = "./data/VOCdevkit/VOC2012/"
train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(
    rootpath=rootpath)

# 나중에 어노테이션 화상만 사용한다

PSPNet을 준비.

from utils.pspnet import PSPNet

net = PSPNet(n_classes=21)

# 학습된 파라미터를 로드
state_dict = torch.load("./weights/pspnet50_30.pth",
                        map_location={'cuda:0': 'cpu'})
net.load_state_dict(state_dict)

print('네트워크 설정 완료: 학습된 가중치를 로드했습니다')

이제 추론을 실행.

원본 화상, PSPNet으로 추론한 어노테이션 화상, 그리고 추론 결과를 원래 화상에 오버랩한 화상까지 총 세 종류를 화면에 그림.

# 1. 원본 화상 표시
image_file_path = "./data/cowboy-757575_640.jpg"
img = Image.open(image_file_path)   # [높이][폭][색RGB]
img_width, img_height = img.size
plt.imshow(img)
plt.show()

# 2. 전처리 클래스 작성
color_mean = (0.485, 0.456, 0.406)
color_std = (0.229, 0.224, 0.225)
transform = DataTransform(
    input_size=475, color_mean=color_mean, color_std=color_std)

# 3. 전처리
# 적당한 어노테이션 화상을 준비하여, 색상 팔레트 정보를 추출
anno_file_path = val_anno_list[0]
anno_class_img = Image.open(anno_file_path)   # [높이][폭]
p_palette = anno_class_img.getpalette()
phase = "val"
img, anno_class_img = transform(phase, img, anno_class_img)

# 4. PSPNet으로 추론한다
net.eval()
x = img.unsqueeze(0)  # 미니 배치화: torch.Size([1, 3, 475, 475])
outputs = net(x)
y = outputs[0]  # AuxLoss 측은 무시, y의 크기는 torch.Size([1, 21, 475, 475])

# 5. PSPNet의 출력으로 최대 클래스를 구해, 색상 팔레트 형식으로 하여, 화상 크기를 원래대로 되돌린다
y = y[0].detach().numpy()  # y: torch.Size([1, 21, 475, 475])
y = np.argmax(y, axis=0)
anno_class_img = Image.fromarray(np.uint8(y), mode="P")
anno_class_img = anno_class_img.resize((img_width, img_height), Image.NEAREST)
anno_class_img.putpalette(p_palette)
plt.imshow(anno_class_img)
plt.show()

# 6. 화상을 투과시켜 겹친다
trans_img = Image.new('RGBA', anno_class_img.size, (0, 0, 0, 0))
anno_class_img = anno_class_img.convert('RGBA')  # 색상 팔레트 형식을 RGBA로 변환

for x in range(img_width):
    for y in range(img_height):
        # 추론 결과 화상의 픽셀 데이터를 취득
        pixel = anno_class_img.getpixel((x, y))
        r, g, b, a = pixel

        # (0, 0, 0)의 배경이라면 그대로 투과시킨다
        if pixel[0] == 0 and pixel[1] == 0 and pixel[2] == 0:
            continue
        else:
            # 그 외 색상은 준비된 화상에 픽셀을 기록한다
            trans_img.putpixel((x, y), (r, g, b, 150))
            # 투과율을 150으로 지정한다

img = Image.open(image_file_path)   # [높이][폭][색RGB]
result = Image.alpha_composite(img.convert('RGBA'), trans_img)
plt.imshow(result)
plt.show()

사람의 다리가 말의 일부로 인식되는 등, 완벽하지는 않지만 대체적으로 화상을 픽셀 수준으로 인식.

시맨틱 분할의 정확성을 높이려면 학습 에폭 수를 늘려야 함.
